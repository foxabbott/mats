{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import plotly_express as px\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "# Model Loading\n",
    "from sae_lens import SAE\n",
    "from sae_lens.analysis.neuronpedia_integration import get_neuronpedia_quick_list\n",
    "\n",
    "# Virtual Weight / Feature Statistics Functions\n",
    "from sae_lens.analysis.feature_statistics import (\n",
    "    get_all_stats_dfs,\n",
    "    get_W_U_W_dec_stats_df,\n",
    ")\n",
    "\n",
    "# Enrichment Analysis Functions\n",
    "from sae_lens.analysis.tsea import (\n",
    "    get_enrichment_df,\n",
    "    manhattan_plot_enrichment_scores,\n",
    "    plot_top_k_feature_projections_by_token_and_category,\n",
    ")\n",
    "from sae_lens.analysis.tsea import (\n",
    "    get_baby_name_sets,\n",
    "    get_letter_gene_sets,\n",
    "    generate_pos_sets,\n",
    "    get_test_gene_sets,\n",
    "    get_gene_set_from_regex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "# this is an outdated way to load the SAE. We need to have feature spartisity loadable through the new interface to remove it.\n",
    "gpt2_saes = {}\n",
    "gpt2_sparsities = {}\n",
    "\n",
    "for layer in range(12):\n",
    "    print(f\"Downloading from layer {layer}\")\n",
    "    sae, original_cfg_dict, sparsity = SAE.from_pretrained(\n",
    "        release=\"gpt2-small-res-jb\",\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    gpt2_saes[f\"blocks.{layer}.hook_resid_pre\"] = sae\n",
    "    gpt2_sparsities[f\"blocks.{layer}.hook_resid_pre\"] = sparsity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 8\n",
    "\n",
    "# get the corresponding SAE and feature sparsities.\n",
    "sparse_autoencoder = gpt2_saes[f\"blocks.{layer}.hook_resid_pre\"]\n",
    "log_feature_sparsity = gpt2_sparsities[f\"blocks.{layer}.hook_resid_pre\"].cpu()\n",
    "\n",
    "W_dec = sparse_autoencoder.W_dec.detach().cpu()\n",
    "\n",
    "# calculate the statistics of the logit weight distributions\n",
    "W_U_stats_df_dec, dec_projection_onto_W_U = get_W_U_W_dec_stats_df(\n",
    "    W_dec, model, cosine_sim=False\n",
    ")\n",
    "W_U_stats_df_dec[\"sparsity\"] = (\n",
    "    log_feature_sparsity  # add feature sparsity since it is often interesting.\n",
    ")\n",
    "display(W_U_stats_df_dec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "\n",
    "# Load the OpenWebText dataset\n",
    "# proportion = 0.000001\n",
    "# dataset = load_dataset(\"openwebtext\", trust_remote_code=True)#, split=f\"train[:{int(proportion * 100)}%]\")\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Function to extract text data from the dataset\n",
    "def extract_texts(dataset, num_samples=None):\n",
    "    texts = []\n",
    "    for i, sample in enumerate(dataset[\"train\"]):\n",
    "        texts.append(sample[\"text\"])\n",
    "        if num_samples and i + 1 >= num_samples:\n",
    "            break\n",
    "    return texts\n",
    "\n",
    "# Extract a large number of texts from the dataset\n",
    "text_data = extract_texts(dataset)  # Adjust num_samples as needed\n",
    "text_data = [i for i in text_data if i != '']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample text data\n",
    "# text_data = [\n",
    "#     \"The quick brown fox jumps over the lazy dog.\",\n",
    "#     \"GPT-2 is a transformer-based model developed by OpenAI.\",\n",
    "#     \"Sparse autoencoders can be used for feature extraction.\"\n",
    "# ]\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "tokenizer = model.tokenizer\n",
    "inputs = tokenizer(text_data, return_tensors=\"pt\", padding=True)\n",
    "print('tokenisation complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # For progress tracking\n",
    "import h5py\n",
    "\n",
    "# num_samples = 100  # Adjust this number as needed\n",
    "# text_data = extract_texts(dataset, num_samples=num_samples)\n",
    "text_data = extract_texts(dataset)\n",
    "text_data = [i for i in text_data if i != '']\n",
    "\n",
    "# Tokenize and encode the text data with truncation\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "# Define max length for truncation\n",
    "# max_length = 512\n",
    "\n",
    "# Hook function to collect hidden states\n",
    "hidden_states = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    hidden_states.append(output)\n",
    "\n",
    "# Attach hooks to each layer\n",
    "hooks = []\n",
    "for layer in range(12):\n",
    "    hook = model.blocks[layer].hook_resid_pre.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Open an HDF5 file to store the latent variables\n",
    "with h5py.File(\"latent_variables.h5\", \"w\") as hdf5_file:\n",
    "\n",
    "    # Initialize datasets for each layer\n",
    "    # Adjust based on the actual dimensionality of the latent variables\n",
    "    latent_size = 24576  # Example size from the output shape\n",
    "    for layer in range(12):\n",
    "        hdf5_file.create_dataset(f\"layer_{layer}\", (0, latent_size), maxshape=(None, latent_size), chunks=True)\n",
    "\n",
    "    # Process the corpus in smaller batches\n",
    "    batch_size = 8  # Adjust batch size based on your GPU/CPU memory capacity\n",
    "    for i in tqdm(range(0, len(text_data), batch_size)):\n",
    "        batch_texts = text_data[i:i + batch_size]\n",
    "        print(\"beginning tokenising\")\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        print(\"finished tokenising\")\n",
    "\n",
    "        # Run the input through GPT-2\n",
    "        with torch.no_grad():\n",
    "            _ = model(inputs['input_ids'])\n",
    "\n",
    "        # Process hidden states\n",
    "        for layer in tqdm(range(12)):\n",
    "            sae = gpt2_saes[f\"blocks.{layer}.hook_resid_pre\"]\n",
    "            hidden_state = hidden_states[layer].detach().cpu()\n",
    "            # print(f\"Hidden state shape for layer {layer}: {hidden_state.shape}\")  # Print hidden state shape\n",
    "\n",
    "            # Extract the final token's representation\n",
    "            final_token_representation = hidden_state[:, -1, :]\n",
    "            # print(f\"Final token representation shape for layer {layer}: {final_token_representation.shape}\")\n",
    "\n",
    "            # Encode the final token's representation using the autoencoder\n",
    "            latent_vars = np.round(sae.encode(final_token_representation).detach().cpu().numpy(), 2)\n",
    "            # print(f\"Latent variable shape for layer {layer}: {latent_vars.shape}\")\n",
    "\n",
    "            # Append latent variables to the HDF5 file\n",
    "            layer_dataset = hdf5_file[f\"layer_{layer}\"]\n",
    "            current_size = layer_dataset.shape[0]\n",
    "            new_size = current_size + latent_vars.shape[0]\n",
    "            layer_dataset.resize(new_size, axis=0)\n",
    "            layer_dataset[current_size:new_size, :] = latent_vars\n",
    "\n",
    "        # Clear hidden states\n",
    "        hidden_states.clear()\n",
    "        # print(i)\n",
    "        # Flush data to disk periodically\n",
    "        # if i % 10 == 0:  # Adjust the frequency of flushing as needed\n",
    "            # hdf5_file.flush()\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "print(\"Latent variables saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook function to collect hidden states\n",
    "hidden_states = []\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    hidden_states.append(output)\n",
    "\n",
    "# Attach hooks to each layer\n",
    "hooks = []\n",
    "for layer in range(12):\n",
    "    hook = model.blocks[layer].hook_resid_pre.register_forward_hook(hook_fn)\n",
    "    hooks.append(hook)\n",
    "\n",
    "# Run the input through GPT-2\n",
    "with torch.no_grad():\n",
    "    _ = model(inputs['input_ids'])\n",
    "\n",
    "# Remove hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text_data))\n",
    "print(inputs['input_ids'].shape)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the latent variables\n",
    "latent_variables = {f\"layer_{layer}\": [] for layer in range(12)}\n",
    "\n",
    "# Run hidden states through the corresponding autoencoders and save latent variables\n",
    "for layer in range(12):\n",
    "    sae = gpt2_saes[f\"blocks.{layer}.hook_resid_pre\"]\n",
    "    hidden_state = hidden_states[layer].detach().cpu()\n",
    "    latent_vars = sae.encode(hidden_state).detach().cpu().numpy()\n",
    "    latent_variables[f\"layer_{layer}\"].append(latent_vars)\n",
    "\n",
    "# Convert latent variables to numpy arrays\n",
    "for layer in range(12):\n",
    "    latent_variables[f\"layer_{layer}\"] = np.concatenate(latent_variables[f\"layer_{layer}\"], axis=0)\n",
    "\n",
    "# Save the latent variables to files (e.g., using np.save)\n",
    "for layer in range(12):\n",
    "    np.save(f\"latent_variables_layer_{layer}.npy\", latent_variables[f\"layer_{layer}\"])\n",
    "\n",
    "print(\"Latent variables saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
